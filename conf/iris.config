params {
    config_profile_description        = 'IRIS profile provided to run nextflow pipelines on the IRIS cluster at Memorial Sloan Kettering Cancer Center (MSKCC)'
    config_profile_contact            = 'Nikhil Kumar (kumarn1@mskcc.org)'
    config_profile_url                = 'https://mskcc-omics-workflows.gitbook.io/omics-workflows'

    // Resource Limits
    max_cpus                          = 52
    max_memory                        = 550.GB
    max_time                          = 7.d

    // Job Submission Options
    preemptable                       = false  // Use preemptable queue for faster submission
    isolated                          = false  // Set to true when you can only use the provided paritition
    group                             = ''     // IRIS group for the job work default path (e.g. /scratch/my_group)
    qos                               = ''     // Set Quality of Service specification for SLURM jobs (e.g. priority)
    partition                         = ''     // SLURM partition (uses $NXF_SLURM_PARTITION or 'cpu' if not set)

    // Path config
    scratch_path                      = '/localscratch'
    work_path                         = '/scratch'
    singularity_library               = '/data1/core006/resources/singularity_image_library'

    // Validation Parameters
    ignore_params_list                = [
        'max_cpus', 'max_memory', 'max_time',
        'preemptable', 'scratch_path', 'work_path',
        'singularity_library', 'isolated', 'group',
        'qos', 'partition', 'scratch', 'ignore_params_list',
        'schema_ignore_params', 'validationSchemaIgnoreParams'
    ]
    schema_ignore_params              = params.ignore_params_list.join(',')
    validationSchemaIgnoreParams      = params.ignore_params_list.join(',')
}

validation {
    ignoreParams                      = params.ignore_params_list
}

// Set sensible defaults
def scratch_dir                       = new File(params.scratch_path)
def work_base                         = new File(params.work_path + "/${params.group}")
params.partition                      = params.partition                                        ?: System.getenv('NXF_SLURM_PARTITION')  ?: 'cpu'
params.scratch                        = scratch_dir.exists()                                    ?  scratch_dir.getPath()                  : "${PWD}/scratch"
workDir                               = work_base.exists() && work_base.getPath() != '/scratch' ?  work_base.getPath() + '/work'          : "${PWD}/work"
cleanup                               = workDir.startsWith('/scratch')                          ?  true                                   : false
def singularity_scratch               = System.getenv('NXF_SINGULARITY_CACHEDIR')               ?: workDir + '/singularity_scratch'
def singularity_library               = System.getenv('NXF_SINGULARITY_LIBRARYDIR')             ?: params.singularity_library

executor {
    name              = 'slurm'
    pollInterval      = 45.s
    queueSize         = 5000
    queueStatInterval = '1 min'
    submitRateLimit   = '95/1min'
    retry.delay       = '1s'
    retry.maxDelay    = '1 min'
}

singularity {
    enabled      = true
    autoMounts   = true
    cacheDir     = singularity_scratch
    libraryDir   = singularity_library
    pullTimeout  = 1.hour
}

// IRIS SLURM Exit Codes:
// 15, 140 = Wall time limit exceeded
// 125, 137 = Out of memory
process {
    arch                              = 'linux/x86_64'
    executor                          = 'slurm'
    resourceLimits                    = [
        cpus: params.max_cpus,
        memory: params.max_memory,
        time: params.max_time
    ]

    _out_of_memory      = { task -> task.previousTrace && (task.previousTrace.exit == 125 || task.previousTrace.exit == 137) }
    _out_of_time        = { task -> task.previousTrace && (task.previousTrace.exit == 15  || task.previousTrace.exit == 140) }
    _cpu_starved        = { task -> task.previousTrace && task.previousTrace['%cpu']  && task.previousTrace['%cpu']  / task.previousTrace.cpus   >= .80 }
    _near_out_of_memory = { task -> task.previousTrace && task.previousTrace.peak_rss && task.previousTrace.peak_rss / task.previousTrace.memory >= .80 }
    _near_out_of_time   = { task -> task.previousTrace && task.previousTrace.realtime && task.previousTrace.realtime / task.previousTrace.time   >= .80 }
    _increase_memory    = { task, multiply, add -> task.previousTrace && task.previousTrace.memory ? (task.previousTrace.memory as nextflow.util.MemoryUnit) + (multiply * task.attempt) + add : task.memory + (multiply * task.attempt) + add }
    _increase_time      = { task, multiply, add -> task.previousTrace && task.previousTrace.time   ? (task.previousTrace.time as nextflow.util.Duration)     + (multiply * task.attempt) + add : task.time   + (multiply * task.attempt) + add }
    _increase_cpu       = { task, multiply, add -> task.previousTrace && task.previousTrace.cpus   ? task.previousTrace.cpus + (multiply * task.attempt) + add : task.cpus + (multiply * task.attempt) + add }

    _get_process_memory = { first_attempt, task ->
        {
            task.attempt == 1
                ? first_attempt
            : task.attempt > 1 && process._out_of_memory(task)
                ? process._increase_memory(task, 10.GB, 0.GB)
            : task.attempt > 1 && process._out_of_time(task)
                ? process._increase_memory(task, 4.GB, 0.GB)
            : task.attempt > 1 && process._near_out_of_memory(task)
                ? process._increase_memory(task, 4.GB, 0.GB)
            : task.attempt > 3
                ? process._increase_memory(task, 0.GB, 10.GB)
                : process._increase_memory(task, 0.GB, 2.GB)
        }
    }
    _get_process_cpus   = { first_attempt, task ->
        {
            task.attempt == 1
                ? first_attempt
            : task.attempt > 1 && process._out_of_time(task)
                ? process._increase_cpu(task, 0, 1)
            : task.attempt > 1 && process._near_out_of_time(task)
                ? process._increase_cpu(task, 0, 1)
            : task.attempt > 1 && process._cpu_starved(task)
                ? process._increase_cpu(task, 0, 2)
            : task.attempt > 3
                ? process._increase_cpu(task, 0, 1)
                : process._increase_cpu(task, 0, 0)
        }
    }
    _get_process_time  = { first_attempt, task ->
        {
            task.attempt == 1
                ? first_attempt
            : task.attempt > 1 && process._out_of_time(task)
                ? process._increase_time(task, 12.h, 0.h)
            : task.attempt > 1 && process._near_out_of_time(task)
                ? process._increase_time(task, 0.h, 12.h)
            : task.attempt > 3
                ? process._increase_time(task, 0.h, 1.d)
                : process._increase_time(task, 0.h, 2.h)
        }
    }

    withLabel: process_single {
        cpus                          = { process._get_process_cpus(1, task) }
        memory                        = { process._get_process_memory(1.GB, task) }
        time                          = { process._get_process_time(4.h, task) }
    }
    withLabel: process_low {
        cpus                          = { process._get_process_cpus(2, task) }
        memory                        = { process._get_process_memory(12.GB, task) }
        time                          = { process._get_process_time(2.h, task) }
    }
    withLabel: process_medium {
        cpus                          = { process._get_process_cpus(6, task) }
        memory                        = { process._get_process_memory(36.GB, task) }
        time                          = { process._get_process_time(8.h, task) }
    }
    withLabel: process_high {
        cpus                          = { process._get_process_cpus(12, task) }
        memory                        = { process._get_process_memory(72.GB, task) }
        time                          = { process._get_process_time(16.h, task) }
    }
    withLabel: process_long {
        cpus                          = { process._get_process_cpus(2, task) }
        memory                        = { process._get_process_memory(12.GB, task) }
        time                          = { process._get_process_time(20.h, task) }
    }
    withLabel: process_high_memory {
        cpus                          = { process._get_process_cpus(6, task) }
        memory                        = { process._get_process_memory(200.GB, task) }
        time                          = { process._get_process_time(8.h, task) }
    }
    withLabel: process_gpu {
        cpus                          = { process._get_process_cpus(6, task) }
        memory                        = { process._get_process_memory(25.GB, task) }
        time                          = { process._get_process_time(8.h, task) }
        accelerator                   = 1
    }
    withLabel: process_gpu_low {
        cpus                          = { process._get_process_cpus(6, task) }
        memory                        = { process._get_process_memory(25.GB, task) }
        time                          = { process._get_process_time(2.h, task) }
        accelerator                   = 1
    }

    queue = {
        if (params.isolated && params.preemptable) {
            return "preemptable,${params.partition}"
        }
        // Only use the set partition when isolated
        else if (params.isolated) {
            return params.partition
        }
        // Short CPU jobs
        else if (task.time <= 2.h && !task.accelerator) {
            return "cpushort,cpu,${params.partition}"
        }
        // Short GPU jobs
        else if (task.accelerator && task.time <= 2.h) {
            return 'gpushort,gpu'
        }
        // GPU jobs
        else if (task.accelerator) {
            return 'gpu'
        }
        // High memory jobs
        else if (task.memory >= 512.GB || task.memory / task.cpus >= 50.GB) {
            return "cpu_highmem,cpu,${params.partition}"
        }
        // Preemptable jobs
        else if (task.attempt < 2 && params.preemptable) {
            return "preemptable,cpu,${params.partition}"
        }
        else {
            return params.partition
        }
    }

    // Cluster Options for GPU and QoS
    clusterOptions = {
        if (task.accelerator && params.qos) {
            return "--qos=${params.qos} --gres=gpu:${task.accelerator.request}"
        }
        else if (task.accelerator) {
            return "--gres=gpu:${task.accelerator.request}"
        }
        else if (params.qos) {
            return "--qos=${params.qos}"
        }
        else {
            return ''
        }
    }

    // Container Options for GPU Support
    containerOptions = {
        if (task.accelerator && workflow.containerEngine == 'singularity') {
            return '--nv'
        }
        else if (task.accelerator && workflow.containerEngine == 'docker') {
            return '--gpus all'
        }
        else {
            return ''
        }
    }

    scratch                           = params.scratch
    cache                             = true  // Use 'lenient' if caches are not working
    beforeScript                      = 'unset R_LIBS; export SINGULARITYENV_TMPDIR=$NXF_SCRATCH; export SINGULARITYENV_TMP=$NXF_SCRATCH'
    maxRetries                        = 3
    errorStrategy                     = { task.attempt < 4 ? 'retry' : 'ignore' }

    publishDir.mode                   = 'copy'
    publishDir.enabled                = { publishDir.path ? true : false }
    input.mode                        = 'symlink'
    stageInMode                       = 'symlink'
    stageOutMode                      = 'copy'
}

workflow.output.mode                  = 'copy'

trace {
    enabled                           = true
}
